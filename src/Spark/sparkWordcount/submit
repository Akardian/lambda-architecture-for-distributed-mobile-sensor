HDFS_CONTAINER=$(docker ps -q -n 1 -f name=lambda_datanode.*)
SPARK_CONTAINER=$(docker ps -q -n 1 -f name=lambda_spark-master.*)

echo Found HDFS datanode: $HDFS_CONTAINER
echo Found SPARK master: $SPARK_CONTAINER
echo
#Save jars to HDFS
#Local   container only
echo Copy jar file to HDFS datanode
docker cp build/libs/Spark-Test-v01-all.jar $HDFS_CONTAINER:tmp/
echo

echo Create jar folder
docker exec $HDFS_CONTAINER hdfs dfs -mkdir -p ../haw/spark-jars
echo

echo Copy jar file from local file system into HDFS
docker exec $HDFS_CONTAINER hdfs dfs -put -f tmp/Spark-Test-v01-all.jar ../haw/spark-jars
docker exec $HDFS_CONTAINER hadoop fs -chown -R haw:hadoop /user/haw
echo

echo Submit jar file to Spark
# Run on a Spark standalone cluster in cluster deploy mode
docker exec $SPARK_CONTAINER /opt/bitnami/spark/bin/spark-submit \
    --class MyKafkaTest \
    --master spark://spark-master:7077 \
    --deploy-mode cluster \
    --executor-memory 3G \
    --total-executor-cores 4 \
    hdfs://namenode:9000/user/haw/spark-jars/Spark-Test-v01-all.jar